{"Qwen 2024 训练策略 超参数配置 官方技术报告":[{"content":"在时间域引入绝对对齐的多模态位置编码（MRoPE），改善模型对事件节奏的把握；; 将预训练语料从1.2 万亿扩展到4.1 万亿tokens，结合高质量监督数据，增强多任务泛化能力。","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1927463592279671080","title":"【Qwen】Qwen2.5-VL技术报告"},{"content":"Qwen-VL在多语言和多模态任务中表现出色。 ... QBench是一个多模态基准测试，它可能包含了多种类型的任务，如图像分类、图像描述等，用于评估模型在多模态任务中的泛化能力。","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/680955430","title":"多模态大模型最新完整综述MM-LLMs"},{"content":"他们通过人工标注、模型生成和策略拼接构建了一组额外的对话数据，以将本地化和多图像理解能力融入到Qwen-VL模型中 · 且确认模型能够有效地将这些能力转移到 ...","doc_type":"web_page","link":"https://blog.csdn.net/v_JULY_v/article/details/145560246","title":"一文通透Qwen多模态大模型：从Qwen-VL、Qwen2-VL(提出 ..."},{"content":"多模态预训练：让模型“看懂世界”，实现图文、视频等不同模态之间的语义统一。 指令微调：像教学生一样教AI理解任务意图，形成结构化的多模态理解能力。","doc_type":"web_page","link":"https://www.xhby.net/content/s687a0f63e4b0fca324a3ac7e.html","title":"小参数也能“硬刚”GTP 4o、Qwen ！悠然无界大模型权威测评 ..."},{"content":"视觉问答任务（VQA）要求模型理解图像的语义内容并回答相关问题，近年来，多模态大语言模型（MLLM）通过融合图像与文本信息，在该领域展现出了强大的推理能力。","doc_type":"web_page","link":"https://www.microsoft.com/en-us/research/articles/new-arrival-in-research-30/","title":"ACL上新| 打造轻量、高效的AI引擎"},{"content":"除这些比较基准，人工智能系统在生成高质量视频方面也取得了重大进展，在某些特定场景下，基. 于语言模型的智能体在时间受限的编程任务中甚至表现优于人类。","doc_type":"web_page","link":"https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf","title":"介绍2025年人工智能指数报告 - Stanford HAI"},{"content":"文心4.5系列开源模型为MoE 架构，是一种创新性的多模态异构模型结构，通过跨模态参数共享机制实现模态间知识融合，同时为各单一模态保留专用参数空间。此架构 ...","doc_type":"web_page","link":"https://ppio.com/ai-computing/llm-api?from=ppinfra","title":"大语言模型API 可靠、可扩展、高性价比"},{"content":"从大语言模型（LLM）的兴起到真正意义的通用人工智能（AGI），还有很多开放性的问题有待解决。我们认为，多模态是从LLM 到AGI 的必经之路。围绕多模态，从智能 ...","doc_type":"web_page","link":"https://www.sensetime.com/cn/news-detail/51169893?categoryId=72","title":"商汤的思考，为何我们如此坚定多模态通用智能？"},{"content":"多模態AI模型與大多數AI模型不同在於，一般大型語言模型(LLM) 僅處理文字資料，而卷積神經網路(CNN) 模型處理影像資料，亦即多數AI模型只能處理單一模態的 ...","doc_type":"web_page","link":"https://www.moea.gov.tw/MNS/doit/industrytech/IndustryTech.aspx?menu_id=13545&it_id=573","title":"多模態AI結合具身智慧發展趨勢"},{"content":"by 王文晟 · 2025 · Cited by 3 — 多模态模型, 尤其是多模态大模型(Large multimodal model, LMM)具有理解图像、场景文本、图表、文档, 以及多语言、多模态理解的强大能力, 可以直接用于具身智能对环境的 ...","doc_type":"web_page","link":"https://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c240542?viewType=HTML","title":"基于大模型的具身智能系统综述"}],"通义千问 2024 MMLU C-Eval 评测结果 基准数据集":[{"content":"通过这种分析，团队为MoE 模型的超参数配置提供了指导，使得经过精细调整激活参数和总参数后，MoE 模型在性能上能够与特定的密集模型变种（例如Qwen2.5-72B 和Qwen2.5-14B）达到 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/13936916587","title":"【LLM技术报告】Qwen2.5技术报告（全文）"},{"content":"by A Yang · 2024 · Cited by 2895 — Abstract:In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs.","doc_type":"web_page","link":"https://arxiv.org/abs/2412.15115","title":"[2412.15115] Qwen2.5 Technical Report"},{"content":"本报告介绍Qwen2.5，这是一系列全面的大语言模型(LLM)，旨在满足多样化的需求。与之前的迭代相比，Qwen 2.5 在预训练和后训练阶段都有显著的改进。","doc_type":"web_page","link":"https://blog.csdn.net/yorkhunter/article/details/144820782","title":"Qwen2.5 技术报告_千问2.5技术报告"},{"content":"预训练. 本节详细介绍预训练数据构建方法、预训练策略细节，并展示基础模型在标准基准测试上的评估结果。 预训练数据. 相比Qwen2.5，Qwen3显著扩展了训练数据的规模和多样性 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1905945819108079268","title":"【LLM技术报告】Qwen3技术报告（全文）"},{"content":"在本报告中，我们介绍了Qwen3，这是Qwen系列模型的最新版本。Qwen3包括一系列大型语言模型（LLMs），旨在提升性能、效率和多语言能力。Qwen3系列包含密集模型和 ...","doc_type":"web_page","link":"https://blog.csdn.net/u013524655/article/details/148933584","title":"通义千问3技术报告原创"},{"content":"与Qwen2.5（杨等，2024b）类似，我们根据上述三个预训练阶段开发了最优超参数（如学习率调度器和批量大小）预测的缩放定律。通过广泛的实验，我们系统地研究了 ...","doc_type":"web_page","link":"https://www.cnblogs.com/emergence/p/18877115","title":"Qwen3 技术报告- 一介布衣"},{"content":"2024 年，美国机构共开发了40 个标志. 性的人工智能模型，而中国只有15 个，欧洲只有3 个。虽然美国在数量上保持领先，但中国的模型在质量上迅速缩小了差距：.","doc_type":"web_page","link":"https://hai.stanford.edu/assets/files/hai_ai_index_report_2025_chinese_version_061325.pdf","title":"介绍2025年人工智能指数报告 - Stanford HAI"},{"content":"与Qwen2.5类似，基于上述三个预训练阶段，开发了用于预测最优超参数（例如学习率调度器和批次大小）的缩放规律。通过广泛的实验，作者系统地研究了模型架构、训练数据、训练阶段 ...","doc_type":"web_page","link":"https://developer.volcengine.com/articles/7508666684266250291","title":"Qwen3 技术报告解读| 开源模型新纪元，全面刷新开源模型极限"},{"content":"通义千问ASR是基于Qwen-Audio训练，专用于语音识别的模型，目前支持的语言有：中文、英文。使用方法请参见实时语音识别。 视觉推理. 2024-12-25. qvq-72b- ...","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/model-announcements","title":"模型相关公告_大模型服务平台百炼(Model Studio) - 阿里云文档"},{"content":"QWEN 是一个综合语言模型系列，包含具有不同参数数量的不同模型。该模型系列包括基本的预训练语言模型、使用人类对齐技术微调的聊天模型，即监督微调（SFT）、 ...","doc_type":"web_page","link":"https://walker-dj1.github.io/QWEN1-%E6%8A%80%E6%9C%AF%E6%8A%A5%E5%91%8A/","title":"QWEN1 技术报告 - DJ Walker"}],"通义千问 Qwen 2024 核心架构 注意力机制优化":[{"content":"... 千亿级参数大模型通义千问2.0。据现场介绍，在10个权威测评中，通义千问2.0综合性能超过GPT-3.5，正在加速追赶GPT-4。以下是通义千问在MMLU、C-Eval ...","doc_type":"web_page","link":"https://blog.csdn.net/aidashuju/article/details/142496107","title":"大模型之基准测试集(Benchmark)-给通义千问2.0做测评的10 ..."},{"content":"2023 年，国内大模型基准测试数据集“井喷式”发. 展，推出包括C-Eval、CMMLU 等评测数据集100 多个，在中文评测. 领域具有显著的影响力。虽然国内提出的基准数据集在数量上有 ...","doc_type":"web_page","link":"https://www.caict.ac.cn/kxyj/qwfb/ztbg/202407/P020240711534708580017.pdf","title":"大模型基准测试体系研究报告(2024 年)"},{"content":"1.常用基准测评集：CMMLU、C-Eval、GSM8K、AGIEval. CMMLU： 综合性的中文评估基准，专门用于评估语言在中文语境下的知识和推理能力。","doc_type":"web_page","link":"https://blog.csdn.net/dwjlyl/article/details/145037762","title":"LLM benchmark简介原创"},{"content":"简介：该模型是针对对金融行业推出的大语言模型，基于通义千问基础模型进行行业语料增量学习，强化金融领域知识和场景应用能力，覆盖金融知识问答、文本分类、信息抽取、文本 ...","doc_type":"web_page","link":"https://github.com/HqWu-HITCS/Awesome-Chinese-LLM","title":"HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的 ..."},{"content":"我们的结果显示，无论是MATH-QWEN-7B-CHAT还是MATH-QWEN-14B-CHAT都优于相同规模的开源模型，并且正在接近与GPT-3.5在数学相关基准数据集（如GSM8K（Cobbe等， ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/685633623","title":"阿里通义千问完整技术报告"},{"content":"对于Qwen-7B-Chat模型，我们同样评测了常规的中文理解（C-Eval）、英文理解（MMLU）、代码（HumanEval）和数学（GSM8K）等权威任务，同时包含了长序列任务的评测结果 ...","doc_type":"web_page","link":"https://huggingface.co/Qwen/Qwen-7B-Chat","title":"Qwen/Qwen-7B-Chat"},{"content":"关于模型基础能力的评测，通义千问团队在MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对Qwen1.5 进行了评估。 在不同模型尺寸下 ...","doc_type":"web_page","link":"https://www.aigcopen.com/content/corporate_news/22161.html","title":"通义千问再开源，Qwen1.5带来六种体量模型"},{"content":"FlagEval 大语言模型评测体系当前包含6 大评测任务，20+评测数据集，80k+评测题目。除了知名的公开数据集HellaSwag、MMLU、C-Eval 等，FlagEval 还集成了包括 ...","doc_type":"web_page","link":"https://www.infoq.cn/article/xgymeqm1lowdukaonoaj","title":"智源研究院FlagEval大模型评测平台更新8月榜单：新增通义 ..."},{"content":"关于模型基础能力的评测，我们在MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对Qwen1. ... 从通义千问网页端和APP的反馈看，用户更加喜爱新 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/691827428","title":"最全的QWen1.5技术报告"},{"content":"关于模型基础能力的评测，我们在MMLU（5-shot）、C-Eval、Humaneval、GS8K、BBH 等基准数据集上对Qwen1.5 进行了评估。 Model, MMLU, C-Eval, GSM8K, MATH ...","doc_type":"web_page","link":"https://qwenlm.github.io/zh/blog/qwen1.5/","title":"Qwen1.5 介绍"}],"Qwen 2024 跨任务泛化能力分析 多模态 语言模型":[{"content":"简介：通义千问是阿里云研发的通义千问大模型系列。其中，Qwen-3是阿里云通义实验 ... 微调代码与ChatMed代码库相同。此外该项目还开源了中医药指令微调数据集 ...","doc_type":"web_page","link":"https://github.com/HqWu-HITCS/Awesome-Chinese-LLM","title":"HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的 ..."},{"content":"AutoTokenizer 是一个自动下载和初始化相应预训练模型所需tokenizer类的工具。这里的 revision='master' 表示使用仓库的主分支版本。 trust_remote_code= ...","doc_type":"web_page","link":"https://blog.csdn.net/China_boy007/article/details/135689714","title":"通义千问的快速开始（其他模型也一样，具体看模型github） 原创"},{"content":"seed (integer): 生成时使用的随机数种子，用于控制模型生成内容的随机性。 stream (boolean): 控制是否使用流式输出。默认为 False 。 stop (string ...","doc_type":"web_page","link":"https://community.modelscope.cn/66fa51dacd8b2677c3c79c91.html","title":"Python调用通义千问qwen2.5模型步骤"},{"content":"用大模型生成可视化的过程通常包括：将用户的查询. （query）和数据整合到提示词（prompt）中，然后使用诸如. Matplotlib 或Seaborn 等可视化库生成代码，最终在沙盒环境 ... 在自然 ...","doc_type":"web_page","link":"https://www.microsoft.com/en-us/research/wp-content/uploads/2025/01/matrix71-678608a015203.pdf","title":"01 焦点02 前沿求索"},{"content":"本项目希望通过开源社区的力量复现Sora，由北大-兔展AIGC联合实验室共同 ... 2024-06-14，通义千问2(Qwen2)大语言模型在PAI-QuickStart的微调、评测与部署实践.","doc_type":"web_page","link":"https://github.com/coderonion/awesome-llm-and-aigc","title":"Awesome-llm-and-aigc"},{"content":"Empowering Smallholder Farmers: AIT Study Highlights Mobile Technology for Climate-Smart Agriculture · Understanding Policy Uncertainty in Global Finance ...","doc_type":"web_page","link":"https://ait.ac.th/search/Zero%20AI%20Review","title":"Zero AI Review"},{"content":"第一点是可拓展的训. 练架构与学习范式：Transformer 架构能够拓展到百亿、千亿甚至万亿参数规模，并. 且将预训练任务统一为预测下一个词这一通用学习范式； ...","doc_type":"web_page","link":"https://llmbook-zh.github.io/LLMBook.pdf","title":"LLMBook.pdf - 大语言模型"},{"content":"在中文预训练权重LLaMA-7B-zh上，使用已有的指令数据集的集合进行指令微调，数据集以及预处理代码整理在Chinese-instruction-dataset。 ... 详见站内专题: 通 ...","doc_type":"web_page","link":"https://wqw547243068.github.io/chatgpt_mimic","title":"ChatGPT复现之路 - 鹤啸九天"},{"content":"问题步骤：. 使用 PDGAN 库创建一个新的GAN模型，然后将模型与原始图像进行交叉验证。 然后对交叉验证的结果进行评估，以确定模型的性能。 如果发现存在性能 ...","doc_type":"web_page","link":"https://ask.csdn.net/questions/8169861","title":"复现图像修复算法PD-GAN模型（基于概率多样性GAN）"}],"通义千问 官方复现 代码库 预处理 随机种子 2024":[{"content":"2、混合注意力机制： · 降低显存占用：通过减少需处理的参数数量，尤其是对于大型模型而言，能够显著降低显存占用。 · 提高效率：不仅节省了内存资源，也加快了 ...","doc_type":"web_page","link":"https://blog.csdn.net/weixin_47970903/article/details/147786407","title":"通义千问2.5--核心改进创新点+主要架构解析原创"},{"content":"基于多语言优化架构，新增27 种语言支持，显著提升代码与数学推理能力，最高承受128K 上下文长度（Qwen2-72B-Instruct）。模型采用分组查询注意力机制 ...","doc_type":"web_page","link":"https://www.cnblogs.com/yfceshi/p/19057284","title":"yfceshi - Qwen2-阿里云最新发布的通义千问开源大模型- 详解"},{"content":"注意力机制优化：如GQA 和窗口注意力，用于降低长文本计算开销。 上下文扩展：如YARN 和DCA，用于扩展模型的上下文窗口。 4. 核心思路. Qwen2.5 的核心 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/14441104741","title":"【论文解读】Qwen2.5 技术报告"},{"content":"Qwen 模型引入了分组查询注意力（GQA） 机制，这种机制能够在性能和效率之间取得平衡。GQA通过张量扩展和KV共享来减少计算复杂度和内存占用，使得大模型的训练 ...","doc_type":"web_page","link":"https://blog.moontak.com/id/540223/","title":"Qwen模型的注意力机制是如何优化的 - 月光AI博客"},{"content":"模型采用分组查询注意力机制，在保持性能的同时降低显存消耗，全面开源于Hugging Face 和ModelScope 平台。 官网资源：https://qwenlm.github.io/zh/blog/ ...","doc_type":"web_page","link":"https://blog.csdn.net/Qynwang/article/details/150581468","title":"Qwen2-阿里云最新发布的通义千问开源大模型原创"},{"content":"基于多语言优化架构，新增27 种语言支持，显著提升代码与数学推理能力，最高支持128K 上下文长度（Qwen2-72B-Instruct）。模型采用分组查询注意力机制 ...","doc_type":"web_page","link":"https://juejin.cn/post/7540592011155357742","title":"Qwen2-阿里云最新发布的通义千问开源大模型"},{"content":"通义千问最新模型Qwen3-Next通过混合注意力与高稀疏MoE架构，在80B参数规模下仅激活3B参数，实现长上下文推理与性价比双突破。 核心内容： 1. 混合注意力架构 ...","doc_type":"web_page","link":"https://www.53ai.com/news/LargeLanguageModel/2025091213546.html","title":"用混合注意力和高稀疏MoE 把训练与推理成本打下来"},{"content":"QKV层是注意力机制的核心部分，通过添加偏差项，可以更好地捕捉输入数据的特征，从而提升模型在处理未知或未见数据时的表现。这种增强外推能力的做法对于处理复杂任务和应对 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/713421330","title":"详解各种LLM系列｜（6）Qwen技术内容详解（万字长文"},{"content":"通义千问是由阿里云自主研发的大模型，用于理解和分析用户输入的自然语言，以及图片、音频、视频等多模态数据。在不同领域和任务为用户提供服务和帮助。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/what-is-qwen-llm","title":"通义千问大语言模型介绍 - 阿里云文档"},{"content":"该结构相比Qwen3的MoE模型结构，进行了以下核心改进：混合注意力机制、高稀疏度MoE结构、一系列训练稳定友好的优化，以及提升推理效率的多token预测机制。","doc_type":"web_page","link":"https://www.cls.cn/detail/2142954","title":"电报解读"}]}