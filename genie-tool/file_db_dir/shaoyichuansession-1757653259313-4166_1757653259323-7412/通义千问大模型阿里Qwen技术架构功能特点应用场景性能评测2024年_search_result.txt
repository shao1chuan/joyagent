{"Qwen大模型 最新进展 局限性分析 2024":[{"content":"超参数优化：. 基于Scaling Law 推导模型超参数，如batch size 和learning rate。 针对不同模型规模和训练数据量，选择最佳超参数。 长文本预训练：. 两 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/14441104741","title":"【论文解读】Qwen2.5 技术报告"},{"content":"... 训练技术和超参数差异巨大，而且往往缺乏详细记录。 然而，我认为深入研究架构本身的结构变化，以了解LLM 开发者在2025 年的最新动态，仍然具有重要 ...","doc_type":"web_page","link":"https://blog.csdn.net/yanqianglifei/article/details/149518829","title":"必读好文：主流大模型架构深度对比，涵盖Llama、Qwen"},{"content":"通过这种分析，团队为MoE 模型的超参数配置提供了指导，使得经过精细调整激活参数和总参数后，MoE 模型在性能上能够与特定的密集模型变种（例如Qwen2.5-72B 和Qwen2.5-14B）达到 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/13936916587","title":"【LLM技术报告】Qwen2.5技术报告（全文）"},{"content":"目标：引入高质量、细粒度标注数据，提升模型的多任务能力，解锁并训练整体模型。 数据集：涵盖7 个任务（说明、VQA、对齐、引用对齐、接地说明、OCR、纯文本自 ...","doc_type":"web_page","link":"https://blog.csdn.net/Python_cocola/article/details/147231983","title":"Qwen-VL系列多模态大模型技术演进-模型架构、训练方法"},{"content":"2024年9月，阿里云在云栖大会上发布了上一代模型Qwen 2.5。Qwen2.5 一次性开源了从0.5B至72B共6个尺寸的全系列模型，覆盖从端侧到云端的全场景 ...","doc_type":"web_page","link":"https://eu.36kr.com/zh/p/3270875152703616","title":"重磅！Qwen 3发布，阿里再度点燃AI开源“篝火”"},{"content":"config.json文件是模型的配置文件，包括了模型框架、训练设置、各种超参数等信息：. Qwen2.5模型参数配置. config.json参数详解和AutoConfig 初始化. 老 ...","doc_type":"web_page","link":"https://www.cnblogs.com/obullxl/p/18508576/NTopic2024102601","title":"transformers 推理Qwen2.5 等大模型技术细节详解(二) ..."},{"content":"阿里云百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/models","title":"模型列表与价格_大模型服务平台百炼(Model Studio) - 阿里云文档"},{"content":"Qwen 团队近日发布了目前最强开源大模型Qwen2 的技术报告。文中介绍了预训练和对齐过程中所采用的技术和方法，最后对模型进行了详细的评估。","doc_type":"web_page","link":"http://139.9.1.231/index.php/2024/09/21/qwen2-technical-report/","title":"Qwen2 技术报告"},{"content":"本文档以通义千问模型的微调操作为例进行说明，使用命令行（Shell）和API （HTTP）两种方式，帮助您使用百炼提供的模型微调功能。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/fine-tuning-api-guide","title":"模型调优操作_大模型服务平台百炼(Model Studio) - 阿里云文档"},{"content":"下表列出了超参数和重要信息，例如预训练token 的数量。特别是，Qwen2-57B-A14B 是从Qwen2-7B 扩展而来的。值得注意的是，与Qwen1.5 模型相比，Qwen2 ...","doc_type":"web_page","link":"https://www.maas.com.cn/blog/448.html","title":"阿里大模型Qwen2技术报告解读"}],"阿里Qwen 模型结构 技术细节 论文 2024":[{"content":"2024年12月阿里发布的Qwen2.5的技术报告：[2412.15115] Qwen2.5 Technical ReportTL;DR研究动机：构建更强大、更通用、更易用的LLM，克服现有模型在 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/14441104741","title":"【论文解读】Qwen2.5 技术报告"},{"content":"本报告介绍Qwen2.5，这是一系列全面的大语言模型(LLM)，旨在满足多样化的需求。与之前的迭代相比，Qwen 2.5 在预训练和后训练阶段都有显著的改进。","doc_type":"web_page","link":"https://blog.csdn.net/yorkhunter/article/details/144820782","title":"Qwen2.5 技术报告_千问2.5技术报告"},{"content":"一、引言今日份LLM详解是介绍另一个中文开源大模型——阿里的通义千问基座大模型Qwen。 本文基于Qwen的技术报告，详解了Qwen从预训练到RLHF对齐的技术内容，并增加一些技术 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/713421330","title":"详解各种LLM系列｜（6）Qwen技术内容详解（万字长文"},{"content":"与Qwen3 Dense模型共享相同的基本架构，参考Qwen2.5-MoE并实现细粒度专家分割(fine-grained expert segmentation)（Dai 等人，2024）。Qwen3 MoE 模型共有128 ...","doc_type":"web_page","link":"https://blog.csdn.net/u014665013/article/details/148496930","title":"Qwen系列之Qwen3解读：最强开源模型的细节拆解原创"},{"content":"Qwen3 的MoE 模型與Qwen3 的dense 模型共享相同的基本架構。實施了細粒度的專家分割(Dai et al., 2024)。Qwen3 MoE 模型共有128 位專家，每個token 激活8 ...","doc_type":"web_page","link":"https://www.themoonlight.io/zh/review/qwen3-technical-report","title":"[论文审查] Qwen3 Technical Report"},{"content":"Qwen 团队近日发布了目前最强开源大模型Qwen2 的技术报告。文中介绍了预训练和对齐过程中所采用的技术和方法，最后对模型进行了详细的评估。","doc_type":"web_page","link":"https://www.53ai.com/news/OpenSourceLLM/2024072306721.html","title":"最强开源大模型？Qwen2 技术报告解读"},{"content":"通义千问是由阿里云自主研发的大模型，用于理解和分析用户输入的自然语言，以及图片、音频、视频等多模态数据。在不同领域和任务为用户提供服务和帮助。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/what-is-qwen-llm","title":"通义千问大语言模型介绍 - 阿里云文档"},{"content":"6月7日，阿里发布了最新、最强大Qwen2大模型。刚刚，Qwen2技术报告（英文）发布。本文是我用Kimi翻译的全文，在公众号后台回复“Qwen2”可下载原文档。","doc_type":"web_page","link":"https://www.sohu.com/a/793657130_121981751","title":"阿里Qwen2技术报告发布：最强开源大模型是如何训练的？"},{"content":"Qwen 2.5的發佈將新增超過100個開源模型，包括基礎模型、指令跟隨模型和擁有多種精度等級及方法的量化模型，涵蓋語言、音頻和視覺等多種模態，以及專門的代碼 ...","doc_type":"web_page","link":"https://itpromag.com/2024/09/20/alicloud-qwen/","title":"阿里雲AI算力大升級！發佈100個開源Qwen 2.5模型及視頻 ..."},{"content":"模型结构方面，研发团队此前做了一系列前期实验验证模型结构设计对效果的影响，整体而言，Google 的PaLM、Meta 的LLaMA 的大多数技术选择都是效果较好的，包括 ...","doc_type":"web_page","link":"https://www.infoq.cn/article/mtayw6a4cnrahgkyxayt","title":"阿里云通义千问14B模型开源：性能超越Llama2等同尺寸模型"}],"Qwen大模型 公开数据集 预处理方法 规模 2024":[{"content":"实证贡献：通过在四大类任务、多种模型（Qwen, LLaMA）上的全面实验 ... 实验证明，GLM-4.5在ARC基准测试中展现了顶尖或接近顶尖的性能，在所有 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1938488775870256759","title":"爱可可AI 前沿推介(8.12)"},{"content":"MiroMind-M1 在AIME24、AIME25 和MATH500 基准测试中，在基于Qwen-2.5 的完全开源7B 模型中取得了SOTA 的性能，32B 模型与开源最强持平。同时，得益于CAMPO ...","doc_type":"web_page","link":"https://www.linkresearcher.com/theses/c8afebb8-b085-4720-8a49-81f303e8ea2e","title":"开源模型也能卷出SOTA！MiroMind-M1高效推理压缩token"},{"content":"研究者观察到，强化学习（RL）技术能显著提升大语言模型（LLMs）的推理能力，特别是Qwen系列模型在数学基准测试上取得了SOTA（顶尖水平）的结果。但奇怪的是，一些研究声称，即使给予 ...","doc_type":"web_page","link":"https://www.xiaoqiedun.com/tags/qwen/","title":"Qwen | 一只小茄墩"},{"content":"关键结论：. 旗舰模型Qwen3-235B-A22B-Base：. 作为MoE 模型（235B 总参数，22B 激活参数），在多数基准中超越DeepSeek-V3 Base、Llama-4-Maverick 等开源模型， ...","doc_type":"web_page","link":"https://blog.csdn.net/wjinjie/article/details/148208426","title":"阿里千问系列：Qwen3技术报告解读（下） 原创"},{"content":"在Qwen、InternLM、LLaMA 等多种模型上，使用仅20 万条Condor 数据SFT 后，主观对话能力显著超越官方RLHF 模型，且不损害知识问答性能。实验还揭示了难度控制 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1937240571572912522","title":"增强大模型对齐能力的新引擎：Condor 知识驱动的数据合成 ..."},{"content":"强大的跨基准性能表现：在多个公开基准测试中的评估表明，在各类生成与编辑任务中均获得SOTA。 基准测试：通用图像生成的GenEval、DPG和OneIG-Bench ...","doc_type":"web_page","link":"https://blog.csdn.net/lonelymanontheway/article/details/148519159","title":"Qwen家族系列模型概述原创"},{"content":"方法論是在一個可控的資料 規模（約31.6K 樣本）上，針對資料流程的每一個步驟進行獨立的消融 實驗，並以模型在下游任務的平均性能作為該策略優劣 的評判標準 ...","doc_type":"web_page","link":"https://www.facebook.com/61572886117067/posts/-ai-%E7%85%89%E4%B8%B9%E6%96%B0%E6%80%9D%E8%B7%AFopenthoughts-%E5%8D%83%E6%AC%A1%E5%AF%A6%E9%A9%97%E6%8F%AD%E7%A4%BA%E6%8E%A8%E7%90%86%E8%B3%87%E6%96%99%E9%9B%86%E6%9C%80%E4%BD%B3%E5%AF%A6%E8%B8%90stanford-berkeley-ucla-%E7%AD%89%E8%B6%85%E9%81%8E-10-%E5%80%8B%E9%A0%82%E5%B0%96%E7%A0%94%E7%A9%B6%E6%A9%9F%E6%A7%8B%E9%80%8F%E9%81%8E%E8%B6%85%E9%81%8E%E5%8D%83%E6%AC%A1/122137421204762870/","title":"AI 煉丹新思路：OpenThoughts 千次實驗揭示推理資料集最佳 ..."},{"content":"... 模型性能远逊于闭源SOTA模型或用专有数据训练的开源模型。为提升开源模型性能 ... 为了评估合成数据对模型性能的影响，我们设计了消融实验（Ablation Study）。","doc_type":"web_page","link":"https://hub.baai.ac.cn/view/40667","title":"智源研究院发布千万级多模态指令数据集Infinity-MM"},{"content":"图表7： Qwen-3-Embedding 模型MTEB 评测跑分. ... 消融实验表明，弱监督预训练和模型融合分别贡献. 约15%和8%的性能提升。Reranker 则直接采用高质量 ...","doc_type":"web_page","link":"https://cnpsec.com/plat_files/upload/png_upload/20250610/202506101749535215303.pdf","title":"谷歌更新Gemini 2.5 Pro，阿里开源Qwen3 新模型"},{"content":"Kimi发布新开源项目Kimi - Audio，为通用音频基础模型。它支持多种任务，在十多个音频基准测试中达SOTA性能，总体排名第一。如LibriSpeech ASR测试WER ...","doc_type":"web_page","link":"https://finance.sina.com.cn/roll/2025-04-27/doc-ineuqiwf7842929.shtml?froms=ggmp","title":"Kimi开源全新音频基础模型，横扫十多项基准测试，总体性能 ..."}],"Qwen大模型 性能评测 SOTA对比 消融实验 基准测试":[{"content":"... 模型，参数规模从5亿到720亿不等。Qwen2.5在Qwen2基础上进行了以下改进：. 在我们最新的大规模数据集上进行预训练，包含多达18万亿个Token。 由于我们 ...","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/what-is-qwen-llm","title":"通义千问大语言模型介绍 - 阿里云文档"},{"content":"3.2.2 预训练方案 · 在第一阶段，仅对ViT进行训练，以提升其与语言模型的对齐能力，为多模态理解奠定坚实基础 · 在第二阶段，所有模型参数都被解冻，模型将在多样 ...","doc_type":"web_page","link":"https://blog.csdn.net/v_JULY_v/article/details/145560246","title":"一文通透Qwen多模态大模型：从Qwen-VL、Qwen2-VL(提出 ..."},{"content":"Qwen 2.5 利用18T tokens 进行预训练，展现了Qwen 系列中最为先进的能力，特别是在领域专长方面，强调了数据规模和模型混合在提升模型能力中的重要性。 *笔者会用GPTs翻译形成 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/13936916587","title":"【LLM技术报告】Qwen2.5技术报告（全文）"},{"content":"本文对训练大规模语言模型所需的预训练数据和微调数据的现状进行了总结，涵盖了数据规模、搜集方式、数据类型及特点、处理流程等，并对当前可用的开源数据集 ...","doc_type":"web_page","link":"https://arxiv.org/html/2411.07715v1","title":"大语言模型训练数据"},{"content":"训练一个大模型（如Qwen-1.8B 或更大规模的模型）是一个复杂且资源密集的过程，涉及多个步骤和工具。以下是完整的训练流程，包括数据准备、模型设计、 ...","doc_type":"web_page","link":"https://blog.csdn.net/weixin_50296887/article/details/145945285","title":"大模型训练流程原创"},{"content":"与Qwen2-VL相比，我们将预训练数据的规模从1.2万亿标记扩展到了大约4万亿标记。预训练数据集的构建方法包括清理原始网络数据、合成数据等。数据集涵盖了多 ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1897297231637357678","title":"Qwen系列解读：回顾Qwen2.5-VL，目前最好的多模态开源 ..."},{"content":"阿里云百炼提供了丰富多样的模型选择，它集成了通义系列大模型和第三方大模型，涵盖文本、图像、音视频等不同模态。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/models","title":"模型列表与价格_大模型服务平台百炼(Model Studio) - 阿里云文档"},{"content":"本研究报告首先回顾了大模型基准测试的发展现状，对已发布的. 主要大模型评测数据集、体系和方法进行了梳理，分析了当前基准测. 试存在的问题和挑战，提出了一套系统化构建大 ...","doc_type":"web_page","link":"https://www.caict.ac.cn/kxyj/qwfb/ztbg/202407/P020240711534708580017.pdf","title":"大模型基准测试体系研究报告(2024 年)"},{"content":"据麦肯锡2024 年度报告，AI 在工业自动化、服务机器人等实体场景的渗透率仍不足15%，其核心瓶颈在于传统模型普遍缺乏三维空间理解、物理规律建模和多模态 ...","doc_type":"web_page","link":"https://www.xhby.net/content/s687a0f63e4b0fca324a3ac7e.html","title":"小参数也能“硬刚”GTP 4o、Qwen ！悠然无界大模型权威测评 ..."},{"content":"其中，Qwen-3是阿里云通义实验室推出的Qwen系列最新一代通用大模型的开源版本，涵盖了多种不同参数规模的模型，包括17亿（1.7B）、40亿（4B）、80亿（8B）、140亿（14B）、320亿（32B） ...","doc_type":"web_page","link":"https://github.com/HqWu-HITCS/Awesome-Chinese-LLM","title":"HqWu-HITCS/Awesome-Chinese-LLM: 整理开源的 ..."}],"Qwen大模型 技术架构 训练策略 超参数配置 2024":[{"content":"2.2.1 模型架构 · Naive动态分辨率. Qwen2-VL 的一项关键架构改进是引入了朴素动态分辨率支持(Dehghani等, 2024)。 · 多模态旋转位置嵌入（M-RoPE） · 统一图像 ...","doc_type":"web_page","link":"https://blog.csdn.net/v_JULY_v/article/details/145560246","title":"一文通透Qwen多模态大模型：从Qwen-VL、Qwen2-VL(提出 ..."},{"content":"qvq-72b-preview模型是由Qwen 团队开发的实验性研究模型，专注于提升视觉推理能力，尤其在数学推理领域。qvq-72b-preview模型的局限性请参见QVQ官方博客。","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/vision/","title":"视觉理解（Qwen-VL） - 阿里云文档"},{"content":"2024年初，国内公司开始快速跟进，deepseek开源了国内首个MOE模型；不久之后，Qwen宣布支持MOE架构。到2024年6月，腾讯推出了当时全球最大的MOE模型Hunyuan- ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/19166953450","title":"2024 LLM回顾，大模型应用与Agent的未来"},{"content":"QWen2-VL于2024年发布（arXiv:2409.12191），是QWen-VL的重大升级，解决了前代模型在处理不同分辨率图像和长视频时的局限性。其GitHub仓库（QwenLM ...","doc_type":"web_page","link":"https://zhuanlan.zhihu.com/p/1929459918538257251","title":"Qwen系列视觉模型演进过程分析：从Qwen-VL到Qwen2.5-VL"},{"content":"它本身的大小，以及它长到这么大的事实，可能暗示着还有其他代码异味，让LLM 难以处理。也许提供一个与其他模型的比较分析会更有见地。","doc_type":"web_page","link":"https://www.reddit.com/r/LocalLLaMA/comments/1ka8ban/qwen_3_unimpressive_coding_performance_so_far/?tl=zh-hans","title":"Qwen 3：到目前为止，编码性能令人印象深刻: r/LocalLLaMA"},{"content":"qwq-32b-preview 模型是由Qwen 团队于2024年开发的实验性研究模型 ... qwq-32b-preview 模型的局限性请参见QwQ官方博客。使用方法 | API参考 ...","doc_type":"web_page","link":"https://help.aliyun.com/zh/model-studio/what-is-qwen-llm","title":"通义千问大语言模型介绍 - 阿里云文档"},{"content":"本文将深入分析Qwen-7B-Chat 模型的主要优势、适用场景、技术局限性以及应对策略，帮助读者更好地理解和应用这一模型。 主体. 模型的主要优势. 性能指标.","doc_type":"web_page","link":"https://blog.csdn.net/gitblog_02663/article/details/144689977","title":"Qwen-7B-Chat 的优势与局限性"},{"content":"2024年最显著的技术进步是模型处理长文本能力的全面提升。仅仅一年前,大多数模型还局限于4096或8192个token的处理能力，只有Claude 2.1例外地支持20万token ...","doc_type":"web_page","link":"https://wallstreetcn.com/articles/3738174","title":"大模型的2024，这可能是最早的一篇年度总结文！"},{"content":"模型的能力和局限性在[Alibaba_Qwen的推文](https://news.miracleplus.com/share_link/49180)中讨论，邀请研究界合作探索其潜力。 **AI基准测试 ...","doc_type":"web_page","link":"https://news.miracleplus.com/share_link/49272","title":"齐思头条2024/11/29「Google DeepMind揭示MoE模型漏洞 ..."},{"content":"第二章百舸争流：中国AI 大模型产业现状及典型案例. 2.1 AI 大模型主要特征. AI 大模型具有泛化性(知识迁移到新领域)、通用性(不局限于特定领域)以. 及涌现性(产生预料之外 ...","doc_type":"web_page","link":"http://download.people.com.cn/jiankang/nineteen17114578641.pdf","title":"2024年中国AI大模型产业发展报告"}]}